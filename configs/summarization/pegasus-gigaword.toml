[dataset]
train_path = "reddit-syac/train.csv"
val_path = "reddit-syac/dev.csv"
test_path = "reddit-syac/test.csv"

[model]
# Model name determines checkpoint and directory and 
name = "pegasus-gigaword"
tokenizer_path = "google/pegasus-gigaword"
model_path = "facebook/pegasus-gigaword"

[preprocessor]
sep_token = '|'
cased = true

[train]

[train.training_args]
num_train_epochs = 20
per_device_eval_batch_size = 8
per_device_train_batch_size = 8
warmup_steps = 500
learning_rate = 5e-4 # from paper
logging_steps = 200
save_strategy = "epoch"
evaluation_strategy = "epoch"
weight_decay = 0.01
save_total_limit = 3

[train.early_stopping]
early_stopping_patience = 8
# early_stopping_threshold = 0.001
